FINAL OPTIMIZATION REPORT
STEP 0: BASELINE TRAINING (freeze)
  - Accuracy: 0.8694
  - Precision: 0.4582
  - Recall: 0.6033
  - F1-Score: 0.5209
  - ROC-AUC: 0.8761
  - Val Loss: 0.2549

STEP 1: FINE-TUNING
Final Metrics:
  - Accuracy: 0.9125
  - Precision: 0.6323
  - Recall: 0.6133
  - F1-Score: 0.6227
  - ROC-AUC: 0.9207
  - Val Loss: 0.3749

STEP 2: HYPERPARAMETER TUNING
Best hyperparameter configuration:
weight_decay     0.00001
batch_size            64
lr                 0.001
momentum             0.8
optimizer          AdamW
accuracy        0.875294
recall          0.756667
precision       0.480932
f1              0.588083
roc_auc         0.920471
val_loss        0.370921
Name: 2, dtype: object
Final Metrics:
  - Accuracy: 0.8753
  - Precision: 0.4809
  - Recall: 0.7567
  - F1-Score: 0.5881
  - ROC-AUC: 0.9205
  - Validation Loss: 0.3709

COMPARISON: Baseline vs Fine-tuned vs Fine Tuned and Hyperparameter Tuned
Baseline F1: 0.5209
Fine Tune F1: 0.6227 (+19.54%)
  After Hyperparameter Tuning F1: 0.5881 (+12.90%)
Baseline Recall: 0.6033
Fine Tune Recall: 0.6133 (+1.66%)
  After Hyperparameter Tuning Recall: 0.7567 (+25.42%)
Best hyperparameter configuration:
  - Learning Rate: 0.001
  - Batch Size: 64
  - Weight Decay: 1e-05
  - Momentum: 0.8
  - Optimizer: AdamW
Final Metrics:
  - Accuracy: 0.8753
  - Precision: 0.4809
  - Recall: 0.7567
  - F1-Score: 0.5881
  - ROC-AUC: 0.9205
  - Validation Loss: 0.3709

