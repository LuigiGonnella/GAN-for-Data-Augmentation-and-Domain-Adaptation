\documentclass[12pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{float}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{subcaption}

% Page geometry
\geometry{margin=1in}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=blue,
}

% Listings setup for code
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{gray!10},
    keywordstyle=\color{blue},
    commentstyle=\color{green!50!black},
    stringstyle=\color{red},
}

% Title information
\title{\textbf{Project Proposal: GANs for Data Augmentation and Domain Adaptation in Medical Imaging}}
\author{}
\date{}

\begin{document}

\maketitle

\tableofcontents
\newpage

\section{Dataset}

\subsection{Source and Description}

\textbf{Dataset Name:} ISIC (International Skin Imaging Collaboration) Dataset

\textbf{Domain:} Medical Imaging - Dermatology / Skin Lesion Classification

\textbf{Source:}
\begin{itemize}
    \item Hospital Clínic de Barcelona
    \item ViDIR Group, Department of Dermatology, Medical University of Vienna
    \item Anonymous contributors
\end{itemize}

\textbf{License:} Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)

\subsection{Dataset Characteristics}

\textbf{Problem Type:} Binary classification of skin lesions
\begin{itemize}
    \item \textbf{Class 0:} Benign lesions
    \item \textbf{Class 1:} Malignant lesions (target for augmentation)
\end{itemize}

\textbf{Dataset Statistics:}
\begin{itemize}
    \item \textbf{Baseline Dataset:}
    \begin{itemize}
        \item Training set: $\sim$11,000 images total
        \begin{itemize}
            \item Benign: $\sim$10,000 images
            \item Malignant: $\sim$1,000 images
        \end{itemize}
        \item \textbf{Imbalance Ratio:} 10:1 (benign:malignant)
        \item Validation and test sets maintained with similar distributions
    \end{itemize}
\end{itemize}

\textbf{Image Specifications:}
\begin{itemize}
    \item \textbf{Resolution:} $128 \times 128$ pixels (resized)
    \item \textbf{Channels:} RGB (3 channels)
    \item \textbf{Format:} PNG
\end{itemize}

\textbf{Domain Challenges:}
\begin{itemize}
    \item \textbf{Class Imbalance:} Severe imbalance with malignant cases being $10\times$ less frequent
    \item \textbf{Data Scarcity:} Limited malignant samples for training robust classifiers
    \item \textbf{Intra-class Variability:} High diversity in lesion appearance, color, shape, and texture
    \item \textbf{Medical Relevance:} Critical need for accurate malignant lesion detection
\end{itemize}

\subsection{Dataset Structure}

\begin{verbatim}
data/
|-- raw/
|   |-- images/                    # Original ISIC images
|   +-- metadata.csv               # Image metadata and labels
|-- processed/
|   |-- baseline/                  # Initial imbalanced dataset
|   |   |-- train/
|   |   |   |-- benign/           (~10,000 images)
|   |   |   +-- malignant/        (~1,000 images)
|   |   |-- val/
|   |   +-- test/
|   |-- augmented/                 # Baseline + GAN-generated samples
|   |   +-- train/
|   |       +-- malignant/        (baseline + ~3,000 synthetic)
|   +-- domain_adaptation/         # Domain shift evaluation
|       |-- source_synthetic/      # Training: synthetic malignant + real benign
|       +-- target_real/           # Testing: real malignant + real benign
+-- synthetic/                     # GAN-generated samples by version
    |-- dcgan_hinge/
    |-- dcgan_mse/
    |-- dcgan_wasserstein/
    +-- cdcgan_*/
\end{verbatim}

\subsection{Augmentation Objectives}

\begin{enumerate}
    \item \textbf{Class Balancing:} Generate synthetic malignant samples to reduce class imbalance
    \item \textbf{Diversity Enhancement:} Increase intra-class diversity for improved generalization
    \item \textbf{Performance Improvement:} Enhance downstream classifier performance on minority class
    \item \textbf{Domain Adaptation:} Evaluate classifier robustness across real-synthetic domain shifts
\end{enumerate}

\subsection{Domain Shift Specification}

\textbf{Objective:} Investigate the domain gap between real and synthetic images and its impact on classifier generalization.

\textbf{Domain Configuration:}
\begin{itemize}
    \item \textbf{Source Domain:} Synthetic malignant images (GAN-generated) + Real benign images
    \item \textbf{Target Domain:} Real malignant images + Real benign images
\end{itemize}

\textbf{Domain Shift Characteristics:}
\begin{enumerate}
    \item \textbf{Class-Specific Shift:} Only malignant class experiences domain shift (synthetic $\rightarrow$ real)
    \item \textbf{Benign Class:} Remains real in both domains (no shift)
    \item \textbf{Asymmetric Challenge:} Tests if synthetic malignant samples can substitute real ones
\end{enumerate}

\textbf{Dataset Split:}
\begin{verbatim}
Source Domain (Training):
  - Benign: 10,000 real images
  - Malignant: 3,000 synthetic (GAN-generated) images
  
Target Domain (Testing):
  - Benign: Real test set (~same distribution as source benign)
  - Malignant: Real test set (domain-shifted from synthetic)
\end{verbatim}

\textbf{Expected Challenges:}
\begin{itemize}
    \item \textbf{Distribution Mismatch:} Synthetic images may not capture all real-world variations
    \item \textbf{Fine-grained Details:} GANs may miss subtle diagnostic features
    \item \textbf{Generalization Gap:} Classifier may overfit to synthetic artifacts
    \item \textbf{Performance Drop:} Expected accuracy reduction on real malignant samples
\end{itemize}

\textbf{Evaluation Metrics:}
\begin{itemize}
    \item Source domain performance (synthetic malignant)
    \item Target domain performance (real malignant)
    \item Domain gap quantification (performance difference)
    \item Per-class accuracy analysis
    \item Confusion matrix comparison across domains
\end{itemize}

\textbf{Domain Adaptation Approach:}
\begin{itemize}
    \item \textbf{Primary Method:} Domain gap evaluation (train on source, test on target)
    \item \textbf{Classifier Loss:} Standard Cross-Entropy Loss
    \item \textbf{Optional Extension:} Domain-Adversarial Neural Network (DANN) could be considered to actively reduce domain gap by:
    \begin{itemize}
        \item Adding a domain discriminator to distinguish source vs target features
        \item Training classifier features to be domain-invariant
        \item Loss formulation: $\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{classification}} + \lambda \times \mathcal{L}_{\text{domain\_adversarial}}$
    \end{itemize}
\end{itemize}

\section{Architecture}

\subsection{Generator Architecture}

\subsubsection{DCGAN Generator (Unconditional)}
\begin{itemize}
    \item \textbf{Architecture Type:} Deep Convolutional GAN (DCGAN)
    \item \textbf{Input:} Random latent vector $\mathbf{z} \in \mathbb{R}^{100}$
    \item \textbf{Output:} $128 \times 128 \times 3$ RGB image
\end{itemize}

\textbf{Parameters:}
\begin{itemize}
    \item Latent dimension: 100
    \item Base filters ($n_1$): 512
    \item Total parameters: $\sim$11.7M
\end{itemize}

\subsubsection{Conditional DCGAN Generator (cDCGAN)}
\begin{itemize}
    \item \textbf{Extension:} Adds class-conditional generation
    \item \textbf{Input:} $\mathbf{z} \in \mathbb{R}^{100}$ + class label embedding
    \item \textbf{Class Embedding:} 2 classes $\rightarrow$ 50-dimensional embedding
    \item \textbf{Concatenated Input:} 150-dimensional vector
\end{itemize}

\subsection{Discriminator Architecture}

\subsubsection{PatchGAN Discriminator (Multiple Loss Variants)}

\textbf{Design Philosophy:}
\begin{itemize}
    \item \textbf{PatchGAN:} Classifies $N \times N$ patches instead of entire image
    \item \textbf{Advantages:} Better captures local texture details critical for medical images
\end{itemize}

\textbf{Architectural Variants:}

\begin{enumerate}
    \item \textbf{PatchGAN with BatchNorm} (for BCE, MSE, Wasserstein losses)
    \begin{itemize}
        \item Batch Normalization after each conv layer (except first)
        \item Dropout: 0.1--0.3
    \end{itemize}

    \item \textbf{PatchGAN with Spectral Normalization} (for Hinge loss)
    \begin{itemize}
        \item Spectral normalization on all conv layers
        \item No Batch Normalization (incompatible with SN)
        \item Dropout: 0.3
    \end{itemize}
\end{enumerate}

\textbf{Parameters:}
\begin{itemize}
    \item Base filters ($n_{df}$): 64
    \item Number of downsampling layers ($n_{\text{layers}}$): 3
    \item Dropout probability: 0.1--0.3 (tuned per loss function)
    \item Output: $7 \times 7$ spatial predictions
\end{itemize}

\subsubsection{Conditional PatchGAN Discriminator}
\begin{itemize}
    \item \textbf{Projection Discriminator:} Class conditioning via projection
    \item \textbf{Class Embedding:} Projects to feature dimension (512)
    \item \textbf{Integration:} Inner product of features with class embedding
    \item \textbf{Output:} Class-conditional patch predictions
\end{itemize}

\section{Training Setup}

\subsection{Loss Functions}

Multiple loss functions are explored and compared:

\subsubsection{Hinge Loss (with Spectral Normalization)}

\textbf{Discriminator Loss:}
\begin{equation}
\mathcal{L}_D = \mathbb{E}[\max(0, 1 - D(\mathbf{x}_{\text{real}}))] + \mathbb{E}[\max(0, 1 + D(\mathbf{x}_{\text{fake}}))]
\end{equation}

\textbf{Generator Loss:}
\begin{equation}
\mathcal{L}_G = -\mathbb{E}[D(G(\mathbf{z}))]
\end{equation}

\textbf{Characteristics:}
\begin{itemize}
    \item Margin-based loss with stable gradients
    \item No saturation issues
    \item Used with Spectral Normalization
    \item Recommended by SN-GAN (Miyato et al., 2018)
\end{itemize}

\subsubsection{Wasserstein Loss with Gradient Penalty}

\textbf{Discriminator Loss:}
\begin{equation}
\mathcal{L}_D = \mathbb{E}[D(\mathbf{x}_{\text{fake}})] - \mathbb{E}[D(\mathbf{x}_{\text{real}})] + \lambda_{\text{GP}} \cdot \text{GP}
\end{equation}
where GP is gradient penalty term

\textbf{Generator Loss:}
\begin{equation}
\mathcal{L}_G = -\mathbb{E}[D(G(\mathbf{z}))]
\end{equation}

\textbf{Hyperparameters:}
\begin{itemize}
    \item $\lambda_{\text{GP}}$: 10 (gradient penalty coefficient)
    \item $n_{\text{critic}}$: 1--2 (discriminator updates per generator update)
\end{itemize}

\subsubsection{Binary Cross-Entropy (BCE) Loss}

\textbf{Discriminator Loss:}
\begin{equation}
\mathcal{L}_D = -\mathbb{E}[\log D(\mathbf{x}_{\text{real}})] - \mathbb{E}[\log(1 - D(\mathbf{x}_{\text{fake}}))]
\end{equation}

\textbf{Generator Loss:}
\begin{equation}
\mathcal{L}_G = -\mathbb{E}[\log D(G(\mathbf{z}))]
\end{equation}

\subsubsection{Mean Squared Error (MSE) Loss}

\textbf{Discriminator Loss:}
\begin{equation}
\mathcal{L}_D = \mathbb{E}[(D(\mathbf{x}_{\text{real}}) - 1)^2] + \mathbb{E}[D(\mathbf{x}_{\text{fake}})^2]
\end{equation}

\textbf{Generator Loss:}
\begin{equation}
\mathcal{L}_G = \mathbb{E}[(D(G(\mathbf{z})) - 1)^2]
\end{equation}

\subsection{Optimization}

\textbf{Optimizer:} Adam

\textbf{Learning Rates} (tuned per loss function):
\begin{itemize}
    \item Generator ($g_{\text{lr}}$): $1 \times 10^{-4}$ to $2 \times 10^{-4}$
    \item Discriminator ($d_{\text{lr}}$): $1 \times 10^{-4}$ to $2 \times 10^{-4}$
    \item Learning rate combinations tested:
    \begin{itemize}
        \item $g_{\text{lr}}=2 \times 10^{-4}$, $d_{\text{lr}}=2 \times 10^{-4}$ (balanced)
        \item $g_{\text{lr}}=1 \times 10^{-4}$, $d_{\text{lr}}=2 \times 10^{-4}$ (slower generator)
        \item $g_{\text{lr}}=2 \times 10^{-4}$, $d_{\text{lr}}=1 \times 10^{-4}$ (faster generator)
        \item $g_{\text{lr}}=1 \times 10^{-4}$, $d_{\text{lr}}=1 \times 10^{-4}$ (conservative)
        \item $g_{\text{lr}}=3 \times 10^{-4}$, $d_{\text{lr}}=3 \times 10^{-4}$ (aggressive)
    \end{itemize}
\end{itemize}

\textbf{Adam Hyperparameters:}
\begin{itemize}
    \item $\beta_1$: 0.5
    \item $\beta_2$: 0.999
\end{itemize}

\textbf{Batch Size:} 32 or 64 (hyperparameter tuned)

\textbf{Training Duration:} 300 epochs (full training) or 25 epochs (hyperparameter tuning)

\textbf{Gradient Clipping:} Applied to both networks (max\_norm=1.0)

\subsection{Hyperparameter Tuning Strategy}

\textbf{Two-Stage Approach:}

\textbf{Stage 1: Learning Rate Tuning}
\begin{itemize}
    \item Grid search over 5 learning rate combinations
    \item 25 epochs per configuration
    \item Metric: FID score (Fréchet Inception Distance)
    \item Select best learning rates for Stage 2
\end{itemize}

\textbf{Stage 2: Architecture Hyperparameter Tuning}
\begin{itemize}
    \item Random search ($N=10$ iterations)
    \item Parameters tuned:
    \begin{itemize}
        \item Latent dimension: [100, 128, 256]
        \item Number of layers ($n_{\text{layers}}$): [2, 3, 4]
        \item Dropout: [0.1, 0.3, 0.5]
        \item Batch size: [32, 64]
        \item $n_{\text{critic}}$: [1, 2]
    \end{itemize}
    \item 25 epochs per configuration
    \item Metric: Minimum FID score achieved
    \item Best configuration selected for final training
\end{itemize}

\textbf{Final Training:}
\begin{itemize}
    \item 300 epochs with best hyperparameters
    \item Generate 3,000 synthetic malignant samples
\end{itemize}

\subsection{Training Stability Techniques}

\begin{enumerate}
    \item \textbf{Spectral Normalization:} Constrains Lipschitz constant (for Hinge loss)
    \item \textbf{Gradient Clipping:} Prevents exploding gradients
    \item \textbf{Gradient Penalty:} Enforces 1-Lipschitz constraint (for Wasserstein)
    \item \textbf{Dropout:} Regularization in discriminator (0.1--0.3)
    \item \textbf{Progressive Monitoring:} Track mode collapse and vanishing gradients
\end{enumerate}

\section{Evaluation Metrics}

\subsection{GAN Quality Metrics}

\subsubsection{Fréchet Inception Distance (FID)}

\textbf{Purpose:} Measures distributional similarity between real and generated images

\textbf{Computation:}
\begin{equation}
\text{FID} = \|\boldsymbol{\mu}_r - \boldsymbol{\mu}_g\|^2 + \text{Tr}(\boldsymbol{\Sigma}_r + \boldsymbol{\Sigma}_g - 2\sqrt{\boldsymbol{\Sigma}_r \cdot \boldsymbol{\Sigma}_g})
\end{equation}
where:
\begin{itemize}
    \item $\boldsymbol{\mu}_r$, $\boldsymbol{\Sigma}_r$: Mean and covariance of real image features
    \item $\boldsymbol{\mu}_g$, $\boldsymbol{\Sigma}_g$: Mean and covariance of generated image features
    \item Features extracted from Inception V3 network
\end{itemize}

\textbf{Interpretation:} Lower is better (closer distributions)

\textbf{Evaluation Frequency:} Every 50 epochs

\textbf{Sample Size:} 256 images (real and fake)

\subsubsection{Inception Score (IS)}

\textbf{Purpose:} Measures quality and diversity of generated images

\textbf{Computation:}
\begin{equation}
\text{IS} = \exp\left(\mathbb{E}_{\mathbf{x}}\left[\text{KL}(p(y|\mathbf{x}) \| p(y))\right]\right)
\end{equation}
where:
\begin{itemize}
    \item $p(y|\mathbf{x})$: Class probabilities from Inception network
    \item $p(y)$: Marginal class distribution
\end{itemize}

\textbf{Interpretation:} Higher is better (more diverse and confident)

\textbf{Output:} Mean $\pm$ standard deviation over 10 splits

\subsubsection{Mode Collapse Detection}

\textbf{Method:} Cosine similarity analysis of Inception features

\textbf{Metrics:}
\begin{itemize}
    \item Mean similarity score
    \item Diversity score = 1 - mean\_similarity
    \item Collapse threshold: 0.7
\end{itemize}

\textbf{Interpretation:} High similarity indicates mode collapse

\subsubsection{Vanishing Gradient Detection}

\textbf{Method:} Analyze loss trajectory changes

\textbf{Window Size:} 10 epochs

\textbf{Threshold:} 0.001

\textbf{Metrics:}
\begin{itemize}
    \item Generator gradient magnitude
    \item Discriminator gradient magnitude
\end{itemize}

\subsection{Classifier Performance Metrics}

\subsubsection{Primary Metrics}

\begin{enumerate}
    \item \textbf{Accuracy:} Overall classification accuracy
    \item \textbf{F1-Score:} Harmonic mean of precision and recall (critical for imbalanced data)
    \item \textbf{Recall (Sensitivity):} True positive rate for malignant detection
    \item \textbf{Precision:} Positive predictive value
    \item \textbf{ROC-AUC:} Area under ROC curve
    \item \textbf{Confusion Matrix:} Detailed breakdown of predictions
\end{enumerate}

\subsubsection{Evaluation Scenarios}

\textbf{Baseline Classifier:}
\begin{itemize}
    \item Trained on original imbalanced dataset (10k benign, 1k malignant)
    \item Establishes performance ceiling with limited data
\end{itemize}

\textbf{Augmented Classifier:}
\begin{itemize}
    \item Trained on baseline + 3,000 GAN-generated malignant samples
    \item Expected improvements:
    \begin{itemize}
        \item Higher recall on malignant class
        \item Better F1-score
        \item Reduced overfitting
    \end{itemize}
\end{itemize}

\textbf{Comparison:}
\begin{itemize}
    \item Improvement in F1-score
    \item Change in recall/precision trade-off
    \item Reduction in false negatives (critical in medical domain)
\end{itemize}

\subsection{Training Monitoring}

\textbf{Tracked Metrics per Epoch:}
\begin{itemize}
    \item Generator loss
    \item Discriminator loss
    \item $D(\text{real})$: Discriminator confidence on real images
    \item $D(\text{fake})$: Discriminator confidence on fake images
\end{itemize}

\textbf{Visualization:}
\begin{itemize}
    \item Loss curves (generator vs discriminator)
    \item Discriminator confidence over time
    \item FID/IS score progression
    \item Sample image grids at checkpoints
\end{itemize}

\textbf{Selection Criteria:}
\begin{enumerate}
    \item Minimum FID score (best match to real distribution)
    \item Highest classifier F1-score improvement
    \item Training stability (no mode collapse)
    \item Visual quality assessment
\end{enumerate}

\section{Experimental Pipeline}

\subsection{Phase 1: Baseline Establishment}
\begin{enumerate}
    \item Train classifier on imbalanced baseline dataset
    \item Evaluate baseline performance metrics
    \item Document limitations (overfitting, poor minority class performance)
\end{enumerate}

\subsection{Phase 2: GAN Training}
\begin{enumerate}
    \item Implement DCGAN and cDCGAN architectures
    \item Test multiple loss functions (Hinge, Wasserstein, BCE, MSE)
    \item Perform hyperparameter tuning:
    \begin{itemize}
        \item Learning rate optimization
        \item Architecture parameter search
    \end{itemize}
    \item Train final models with best configurations
    \item Generate 3,000 synthetic malignant samples per configuration
\end{enumerate}

\subsection{Phase 3: Augmented Training}
\begin{enumerate}
    \item Combine baseline + synthetic malignant samples
    \item Train classifiers on augmented datasets
    \item Evaluate performance improvements
    \item Compare across different GAN variants
\end{enumerate}

\subsection{Phase 4: Domain Adaptation Evaluation}

\begin{enumerate}
    \item \textbf{Dataset Preparation:}
    \begin{itemize}
        \item Source domain: Synthetic malignant + real benign (training)
        \item Target domain: Real malignant + real benign (testing)
    \end{itemize}
    
    \item \textbf{Training Strategy:}
    \begin{itemize}
        \item Train classifier exclusively on source domain
        \item No access to real malignant samples during training
    \end{itemize}
    
    \item \textbf{Evaluation:}
    \begin{itemize}
        \item Test on target domain (real malignant samples)
        \item Measure domain gap: performance drop from source to target
        \item Analyze failure modes and misclassifications
    \end{itemize}
    
    \item \textbf{Domain Gap Analysis:}
    \begin{itemize}
        \item Compare source vs target accuracy
        \item Per-class performance breakdown
        \item Confusion matrices for both domains
        \item Feature space visualization (t-SNE/UMAP)
        \item Error analysis on domain-shifted samples
    \end{itemize}

    \item \textbf{Comparison Across GAN Variants:}
    \begin{itemize}
        \item Which GAN loss produces most domain-robust synthetic data?
        \item Correlation between FID score and domain gap
        \item Trade-off between augmentation benefit and domain transfer
    \end{itemize}
\end{enumerate}

\subsection{Phase 5: Analysis and Reporting}

\begin{enumerate}
    \item Statistical comparison of metrics across all scenarios:
    \begin{itemize}
        \item Baseline (real imbalanced data)
        \item Augmented (real + synthetic)
        \item Domain shift (synthetic malignant training $\rightarrow$ real malignant testing)
    \end{itemize}
    \item Visual quality assessment
    \item Domain adaptation analysis and insights
    \item Final recommendations and conclusions
\end{enumerate}

\end{document}
